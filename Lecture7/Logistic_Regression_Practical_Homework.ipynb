{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### NOTE! This algorithm is not yet finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset \n",
    "load diabetes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "diabetes_df = pd.read_csv('diabetes_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FEATURE SELECTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. A function which drops columns if they have zeros more than some threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def drop_cols_with_zeros(df, threshold=0.9):\n",
    "    new_df = df.replace({0:np.nan})\n",
    "    new_df.dropna(thresh=0.9*len(diabetes_df), axis='columns', inplace=True)    \n",
    "    return new_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. A function which replaces zeros in columns with means of those columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def replace_zeros_with_means(df):\n",
    "    for feature in df.columns:\n",
    "        nonzero_mean = df.loc[X[feature]!=0, feature].mean()\n",
    "        df.loc[df[feature]==0, feature] = nonzero_mean\n",
    "        df[feature].fillna(nonzero_mean, inplace=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. A function which finds columns whose correlation with target columns is greater than given threshold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def find_best_features(X,y, threshold=0.2):\n",
    "    columns_to_drop = []\n",
    "    for column in X.columns:\n",
    "        if abs(np.corrcoef(X[column].values, y.values)[0,1])<threshold:\n",
    "            columns_to_drop.append(column)\n",
    "    return X[X.columns.difference(columns_to_drop)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applying feature selection to columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "target_col = 'Outcome'\n",
    "y = diabetes_df[target_col]\n",
    "X_original = diabetes_df[diabetes_df.columns.difference([target_col])]\n",
    "X = drop_cols_with_zeros(X_original)\n",
    "X = replace_zeros_with_means(X)\n",
    "X = find_best_features(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Divide the dataset into train and test datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement the following functions to create your own logistic regression algorithm from scratch.\n",
    "#### Feel free to use more additinal functions in your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NOTE! Since data juggling between several function, especially in Jupyter Notebook, is a little bit annoying, I've incapsulated all in one class similar as in Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class SimpleLogisticRegression:\n",
    "\n",
    "    def fit(self, X, Y, threshold=0.5, learning_rate=1e-4):\n",
    "        # threshold is value by which outcome probabilities are binarizied\n",
    "        self.threshold = threshold\n",
    "        self.gradient_descent(X.values, Y.values, step_size=learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return pd.Series(np.where(self.sigmoid(X, self.beta) > self.threshold, 1, 0))\n",
    "\n",
    "    def get_beta(self):\n",
    "        return self.beta\n",
    "\n",
    "    def sigmoid(self, X, beta):\n",
    "        \"\"\"\n",
    "        :param X: data matrix (2 dimensional np.array)\n",
    "        \"\"\"\n",
    "        dot = np.dot(X, beta)\n",
    "        return 1 / (1 + np.exp(-dot))\n",
    "\n",
    "    def gradient(self, beta, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: data matrix (2 dimensional np.array)\n",
    "        :param Y: response variables (1 dimensional np.array)\n",
    "        :param beta: value of beta (1 dimensional np.array)\n",
    "        :return: np.array i.e. gradient according to the data\n",
    "        \"\"\"\n",
    "        return np.dot(self.sigmoid(X, beta) - Y, X)\n",
    "\n",
    "    def cost_func(self, X, Y, beta):\n",
    "        \"\"\"\n",
    "        :param X: data matrix (2 dimensional np.array)\n",
    "        :param Y: response variables (1 dimensional np.array)\n",
    "        :param beta: value of beta (1 dimensional np.array)\n",
    "        :return: numberic value of the cost function\n",
    "\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        hypoth = self.sigmoid(X, beta)\n",
    "        cost = 1 / n * (np.dot(-Y, np.log(hypoth)) - np.dot( (1 - Y),  np.log(1 - hypoth)))\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, X, Y, epsilon=1e-6, step_size=1e-4, max_steps=10_000):\n",
    "        \"\"\"\n",
    "        :param X: data matrix (2 dimensional np.array)\n",
    "        :param Y: response variables (1 dimensional np.array)\n",
    "        :param epsilon: threshold for a change in cost function value\n",
    "        :param max_steps: maximum number of iterations before algorithm will terminate.\n",
    "        \"\"\"\n",
    "        beta = np.zeros(X.shape[1] + 1)\n",
    "        # adding columns with ones to X\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        step = 1\n",
    "        while step < max_steps:\n",
    "            beta -= step_size * self.gradient(beta, X, Y)\n",
    "            if np.abs(self.cost_func(X, Y, beta)) < epsilon:\n",
    "                print('Gradient descent converged on {}-th step'.format(str(step)))\n",
    "                break\n",
    "            step += 1\n",
    "        else:\n",
    "            print('Can not converge gradient descent with given epsilon, step size and maximum steps')\n",
    "            print('Last value of beta was ', str(beta))\n",
    "\n",
    "        self.beta = beta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run logistic regression using the features of your choice and using \"Outcome\" as a target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/karen/anaconda3/envs/ml_beginner_course/lib/python3.7/site-packages/ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in log\n/home/karen/anaconda3/envs/ml_beginner_course/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Can not converge gradient descent with given epsilon, step size and maximum steps\nLast value of beta was  [-203.54551284   -0.46034571   -0.83588061    1.94984376]\nAccuracy of my classifier:  0.6298701298701299\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "my_clf = SimpleLogisticRegression()\n",
    "my_clf.fit(X_train, y_train)\n",
    "\n",
    "X_array = np.insert(X_test.values, 0, 1, axis=1)\n",
    "\n",
    "my_y_pred = my_clf.predict(X_array)\n",
    "\n",
    "print('Accuracy of my classifier: ', accuracy_score(y_test, my_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the logistic regression available in Sklearn on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy of sklearn regressor:  0.7337662337662337\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "sklearn_clf = LogisticRegression()\n",
    "sklearn_clf.fit(X_train, y_train)\n",
    "y_pred = sklearn_clf.predict(X_test)\n",
    "print('Accuracy of sklearn regressor: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extra: try to plot the results of your algorithm i.e. a scatter plot of points classified into 2 classes in different colors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}